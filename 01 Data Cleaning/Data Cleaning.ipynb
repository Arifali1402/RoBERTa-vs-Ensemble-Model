{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM42WShpKl0Uumf0Yf64oDl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XGIW8EIUEDAI","executionInfo":{"status":"ok","timestamp":1742315262845,"user_tz":-330,"elapsed":912,"user":{"displayName":"Arkapratim Ghosh","userId":"13470667593128632597"}},"outputId":"50a652e8-9f78-4cd3-b55a-40577fb19a7b"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"stream","name":"stdout","text":["                                                Text  \\\n","0  Excited to have Larry Ellison &amp; Kathleen W...   \n","1  When one of the interns working on the simulat...   \n","2                           @5AllanLeVito Got it ðŸ˜€ðŸ‡ºðŸ‡¦   \n","3  Yes. Supercharger coverage will extend to 100%...   \n","4                     @demishassabis Congratulations   \n","\n","                                        cleaned_text  \n","0  excited larry ellison amp kathleen wilsonthomp...  \n","1          one interns working simulation drops ball  \n","2                                                got  \n","3  yes supercharger coverage extend europe next y...  \n","4                                    congratulations  \n"]}],"source":["import pandas as pd\n","import re\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","import nltk\n","\n","# Download required NLTK resources\n","nltk.download('stopwords')\n","nltk.download('punkt_tab')\n","\n","# Load the dataset from tweets.csv\n","df = pd.read_csv('tweets.csv')\n","\n","# Keep only the 'Text' column\n","df = df[['Text']]\n","\n","# Remove rows with missing text\n","df.dropna(subset=['Text'], inplace=True)\n","\n","# Remove duplicate rows based on 'Text'\n","df.drop_duplicates(subset=['Text'], inplace=True)\n","\n","# Define a set of negative words to retain during stopword removal\n","negative_words = {\"not\", \"no\", \"nor\", \"never\", \"nothing\", \"nowhere\",\n","                  \"neither\", \"cannot\", \"n't\", \"without\", \"barely\", \"hardly\", \"scarcely\"}\n","\n","def clean_text(text):\n","    # Remove URLs\n","    text = re.sub(r'http\\S+', '', text)\n","    # Remove mentions (@username)\n","    text = re.sub(r'@\\w+', '', text)\n","    # Remove special characters, numbers, and punctuations (keeping only letters and spaces)\n","    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n","    # Convert text to lowercase\n","    text = text.lower()\n","    # Tokenize the text\n","    tokens = word_tokenize(text)\n","    # Remove stopwords but retain the defined negative words\n","    tokens = [word for word in tokens if word not in stopwords.words('english') or word in negative_words]\n","    # Join tokens back into a cleaned string\n","    return ' '.join(tokens)\n","\n","# Apply the cleaning function to the 'Text' column\n","df['cleaned_text'] = df['Text'].apply(clean_text)\n","\n","# Display the first few rows of the cleaned dataset\n","print(df.head())\n","\n","# Save the cleaned dataset to a new CSV file (optional)\n","df.to_csv('preprocessed_tweets.csv', index=False)\n"]}]}