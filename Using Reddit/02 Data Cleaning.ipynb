{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMqvxMhSYw6VYPXZ7CySZx6"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nzml0TNPRhOe","executionInfo":{"status":"ok","timestamp":1746613648027,"user_tz":-330,"elapsed":2444,"user":{"displayName":"ARIF ALI","userId":"03439795110802964455"}},"outputId":"5f7be685-8caa-4740-b0c0-d1e351a2480e"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"]},{"output_type":"stream","name":"stdout","text":["   SNO SUBREDDIT_NAME                                       cleaned_text  \\\n","0    1   interstellar  monthly interstellar showings megathread greet...   \n","1    2   interstellar  new rule no photos videos theatrical screening...   \n","2    3   interstellar      look docking scene brand cooper cooper buying   \n","3    4   interstellar  stop spin observe wormhole revert back zero gr...   \n","4    5   interstellar  come tars know deliberate reference brightened...   \n","\n","   UPVOTES  DOWNVOTES  \n","0        5          0  \n","1      362          0  \n","2      238          0  \n","3      117          0  \n","4      122          0  \n"]}],"source":["import pandas as pd\n","import re\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize\n","\n","# 1. Download NLTK resources (run once)\n","nltk.download('stopwords')\n","nltk.download('punkt')\n","nltk.download('punkt_tab')\n","\n","# 2. Load your scraped data\n","df = pd.read_csv('interstellar_posts_with_votes.csv')\n","\n","# 3. Keep only the columns you need for text analysis\n","df = df[['SNO', 'SUBREDDIT_NAME', 'TEXT', 'DATE', 'UPVOTES', 'DOWNVOTES']]\n","\n","# 4. Drop rows where TEXT is missing\n","df.dropna(subset=['TEXT'], inplace=True)\n","\n","# 5. Remove exact duplicate posts\n","df.drop_duplicates(subset=['TEXT'], inplace=True)\n","\n","# 6. Define negative words to retain\n","negative_words = {\n","    \"not\", \"no\", \"nor\", \"never\", \"nothing\", \"nowhere\",\n","    \"neither\", \"cannot\", \"n't\", \"without\", \"barely\", \"hardly\", \"scarcely\"\n","}\n","\n","# 7. Cleaning function\n","def clean_text(text):\n","    # (a) Remove URLs\n","    text = re.sub(r'http\\S+', '', text)\n","    # (b) Remove u/ or r/ mentions\n","    text = re.sub(r'/?[ur]\\/\\w+', '', text)\n","    # (c) Keep only letters and spaces\n","    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n","    # (d) Lowercase\n","    text = text.lower()\n","    # (e) Tokenize\n","    tokens = word_tokenize(text)\n","    # (f) Remove stopwords except negative words\n","    sw = set(stopwords.words('english'))\n","    tokens = [tok for tok in tokens if (tok not in sw or tok in negative_words)]\n","    # (g) Remove any leftover one-letter tokens (optional)\n","    tokens = [tok for tok in tokens if len(tok) > 1]\n","    return ' '.join(tokens)\n","\n","# 8. Apply cleaning\n","df['cleaned_text'] = df['TEXT'].apply(clean_text)\n","\n","# 9. (Optional) Remove rows where cleaned_text is empty\n","df = df[df['cleaned_text'].str.strip() != '']\n","\n","# 10. Inspect the first few rows\n","print(df[['SNO','SUBREDDIT_NAME','cleaned_text','UPVOTES','DOWNVOTES']].head())\n","\n","# 11. Save to a new CSV\n","df.to_csv('interstellar_posts_preprocessed.csv', index=False)\n"]}]}